# -*- coding: utf-8 -*-
# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
https://colab.research.google.com/drive/1tV0gGZJsNsSd-xUKDB0Luos2fTMqJDy1
"""

# !git clone https://github.com/monisha-jega/WeatherGov.git

# !ls WeatherGov

"""### **DEFINITIONS**"""

import tensorflow as tf 
import numpy as np
import os, argparse
from tensorflow.python.layers import core as layers_core

root_folder = "WeatherGov/"
train_input = root_folder  + "train/train.combined"
train_target = root_folder  + "train/summaries.txt"
valid_input = root_folder  + "dev/dev.combined"
valid_target = root_folder  + "dev/summaries.txt"
test_input = root_folder  + "test/test.combined"

pad_symbol = "pad"
pad_symbol_id = 0
start_symbol = "start"
start_symbol_id = 1
end_symbol = "end"
end_symbol_id = 2
unk_symbol = "unk"
unk_symbol_id = 3


def ones(till_index, length):
	v = [0 for i in range(length)]
	for i in range(till_index):
		v[i] = 1
	return v


# def one_hot(index, vocab_size):
# 	v = [0 for i in range(vocab_size)]
# 	v[index] = 1
# 	return np.array(v)

def one_hot(a, ncols):
  out = np.zeros( (a.size,ncols), dtype=np.uint8)
  out[np.arange(a.size),a.ravel()] = 1
  out.shape = a.shape + (ncols,)
  return out



def make_vocab(tokens):
	word_to_id = {pad_symbol : 0, start_symbol : 1, end_symbol : 2, unk_symbol : 3}
	vocab_count = 4
	for line in tokens:
		for word in line:
			if word not in word_to_id:
				word_to_id[word] = vocab_count	
				vocab_count += 1						
	return word_to_id

def pad(line, max_len):
	pad_length = max_len - len(line)	
	for i in range(pad_length):
		line.append(pad_symbol)
	return line

"""### **LOAD TRAIN INPUT DATA**"""

###################### PROCESS TRAIN INPUT DATA #########################################
train_input_lines = open(train_input, 'r').readlines()[:10]				#Read
train_input_tokens = [line.strip().split() for line in train_input_lines] 	#Tokenize

#Find max length
# max_input_length = -1
# for line in train_input_tokens:
# 	line_length = len(line) + 2
# 	max_input_length = max(max_input_length, line_length)
max_input_length = 150

#Make vocabulary
input_word_to_id = make_vocab(train_input_tokens)			
input_vocab_size = len(input_word_to_id)
#input_id_to_word = {word_id : word for (word, word_id) in input_word_to_id.iteritems()}
#print(input_vocab_size)

#Pad lines to maximum length
for line in train_input_tokens:
  line.insert(0, start_symbol)
  line.append(end_symbol)
  line = pad(line, max_input_length)
  
  
#Convert to id
train_input_tokens_id = [[input_word_to_id.get(word, unk_symbol_id) for word in line] for line in train_input_tokens]
train_input_tokens_id = np.array(train_input_tokens_id)

train_input_tokens_one_hot = one_hot(train_input_tokens_id, input_vocab_size)
print(train_input_tokens_one_hot.shape)
####################################################################################

"""### ***LOAD TRAIN TARGET DATA***"""

####################### PROCESS TRAIN TARGET DATA #########################################
train_target_lines = open(train_target, 'r').readlines()[:10]				#Read
train_target_tokens = [line.strip().split() for line in train_target_lines] #Tokenize

#Find max length
train_target_lengths = []
# max_target_length = -1
for line in train_target_tokens:
	line_length = len(line) + 1
	train_target_lengths.append(line_length)
# 	max_target_length = max(max_target_length, line_length)
# train_target_lengths = np.array(train_target_lengths)
max_target_length = 100
#train_target_lengths = np.ones((len(train_target_tokens)), dtype=int) * max_target_length
train_target_weights = np.array([ones(l, max_target_length) for l in train_target_lengths])
#print("Target wights", train_target_weights.shape)

#Make vocabulary	
target_word_to_id = make_vocab(train_target_tokens)
target_vocab_size = len(target_word_to_id)
target_id_to_word = {word_id : word for (word, word_id) in target_word_to_id.iteritems()}
#print(target_vocab_size)


#Pad lines to maximum length
for line in train_target_tokens:
  line.insert(0, start_symbol_id)
  line = pad(line, max_target_length)

#Convert to ids
train_target_tokens_id = [[target_word_to_id.get(word, unk_symbol_id) for word in line] for line in train_target_tokens]
train_target_tokens_id = np.array(train_target_tokens_id)
#print(train_target_tokens_id.shape)

#Convert to one-hot
train_target_tokens_one_hot = one_hot(train_target_tokens_id, target_vocab_size)
print(train_target_tokens_one_hot.shape)

#Decoder Inputs
#train_target_tokens_id shifted by 1 to the left, first element being "start"
shifted_train_target_tokens_id = []
for instance in train_target_tokens_id:
	shifted =  []
	for word_id in instance[:-1]:
		shifted.append(word_id)
	shifted.append(end_symbol_id)
	shifted_train_target_tokens_id.append(shifted)
shifted_train_target_tokens_id = np.array(shifted_train_target_tokens_id)
#print(shifted_train_target_tokens_id.shape)
##############################################################################

"""### **LOAD VALID DATA**"""

####################### PROCESS VALID INPUT DATA #########################################
valid_input_lines = open(valid_input, 'r').readlines()[:10]					#Read
valid_input_tokens = [line.strip().split() for line in valid_input_lines] 	#Tokenize

#Find max length
# max_input_length = -1
# for line in valid_input_tokens:
# 	line_length = len(line) + 2
# 	max_input_length = max(max_input_length, line_length)
max_input_length = 150

#Pad lines to maximum length
for line in valid_input_tokens:
	line.insert(0, start_symbol)
	line.append(end_symbol)	
	line = pad(line, max_input_length)
#print(valid_input_tokens.shape)

#Convert to ids
valid_input_tokens_id = [[input_word_to_id.get(word, unk_symbol_id)for word in line] for line in valid_input_tokens]
valid_input_tokens_id = np.array(valid_input_tokens_id)
#print(valid_input_tokens_id.shape)

#Convert to one-hot
valid_input_tokens_one_hot = one_hot(valid_input_tokens_id, input_vocab_size)
print(valid_input_tokens_one_hot.shape)

####################################################################################


####################### PROCESS VALID TARGET DATA #########################################
valid_target_lines = open(valid_target, 'r').readlines()[:10]			#Read
valid_target_tokens = [line.strip().split() for line in valid_target_lines] #Tokenize

#Find max length
valid_target_lengths = []
# max_target_length = -1
for line in valid_target_tokens:
	line_length = len(line) + 1
	valid_target_lengths.append(line_length)
# 	max_target_length = max(max_target_length, line_length)
# valid_target_lengths = np.array(valid_target_lengths)
max_target_length = 100
#valid_target_lengths = np.ones((len(valid_target_tokens)), dtype=int) * max_target_length
valid_target_weights = np.array([ones(l, max_target_length) for l in valid_target_lengths])
#print(valid_target_weights.shape)

#Pad lines to maximum length
for line in valid_target_tokens:
	line.insert(0, start_symbol_id)
	line = pad(line, max_target_length)
#print(valid_target_tokens.shape)

#Convert to ids
valid_target_tokens_id = [[target_word_to_id.get(word, unk_symbol_id)for word in line] for line in valid_target_tokens]
valid_target_tokens_id = np.array(valid_target_tokens_id)
#print(valid_target_tokens_id.shape)

#Convert to one-hot
valid_target_tokens_one_hot = one_hot(valid_target_tokens_id, target_vocab_size)
print(valid_target_tokens_one_hot.shape)

#Decoder Inputs
#valid_target_tokens_id shifted by 1 to the left, last element being "end"
shifted_valid_target_tokens_id = []
for instance in valid_target_tokens_id:
	shifted =  []
	for word_id in instance[1:]:
		shifted.append(word_id)
	shifted.append(end_symbol_id)
	shifted_valid_target_tokens_id.append(shifted)
shifted_valid_target_tokens_id = np.array(shifted_valid_target_tokens_id)
##############################################################################

"""### **LOAD TEST DATA**"""

####################### PROCESS TEST INPUT DATA #########################################
test_input_lines = open(test_input, 'r').readlines()[:10]					#Read
test_input_tokens = [line.strip().split() for line in test_input_lines] 	#Tokenize

#Find max length
# max_input_length = -1
# for line in test_input_tokens:
# 	line_length = len(line) + 2
# 	max_input_length = max(max_input_length, line_length)
max_input_length = 150


#Pad lines to maximum length
for line in test_input_tokens:
	line.insert(0, start_symbol)
	line.append(end_symbol)	
	line = pad(line, max_input_length)
#print(test_input_tokens.shape)

#Convert to id
test_input_tokens_id = [[input_word_to_id.get(word, unk_symbol_id) for word in line] for line in test_input_tokens]
test_input_tokens_id = np.array(test_input_tokens_id)

#Convert to one-hot
test_input_tokens_one_hot = one_hot(test_input_tokens_id, input_vocab_size)
print(test_input_tokens_one_hot.shape)

###################################################################################

"""## **PARAMETERS**"""

# input_vocab_size = len(input_word_to_id)
# target_vocab_size = len(target_word_to_id)
# max_input_length = 100
# max_target_length = 30
bidirectional = True
decode_method = "greedy"
beam_width = 3
inembsize = 256
encsize = 512
decsize = 512
outembsize = 256
max_gradient_norm = 5
learning_rate = 0.001
batch_size = 2
attention = False
epochs = 20
anneal = False
early_stopping = False
dropout_prob = 0.0
save_dir = ""

"""### **MAKE TF GRAPH**"""

####################### PLACEHOLDERS #########################################
tokens = tf.placeholder(tf.int32, [None, max_input_length])
summaries_dec = tf.placeholder(tf.int32, [None, max_target_length])
summary_lengths = tf.placeholder(tf.int32, [None])
summaries_loss = tf.placeholder(tf.int32, [None, max_target_length])
target_weights = tf.placeholder(tf.float32, [None, max_target_length])
lr = tf.placeholder(tf.float32)
num_instances = tf.placeholder(tf.int32)
##############################################################################


#INPUT EMBEDDING
# embedding_layer = tf.layers.dense(inputs=tokens, units=inembsize, activation=tf.nn.tanh)
input_embeddings = tf.Variable(tf.random_uniform([input_vocab_size, inembsize], -1.0, 1.0), dtype=tf.float32)
tokens_embedded = tf.nn.embedding_lookup(input_embeddings, tokens)


####################### ENCODER ########################################
with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):
	if(bidirectional == False):
		# Build RNN cell
		encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(encsize, name="e")

		# Run Dynamic RNN
		encoder_output, encoder_state = tf.nn.dynamic_rnn(
		encoder_cell, tokens_embedded, time_major=False, dtype=tf.float32)

	else:
		# Construct forward and backward cells
		forward_cell = tf.nn.rnn_cell.BasicLSTMCell(encsize, name="f")
		backward_cell = tf.nn.rnn_cell.BasicLSTMCell(encsize, name="b")

		# _, encoder_state = tf.nn.dynamic_rnn(
		# forward_cell, embedding_layer, time_major=False, dtype=tf.float32)
		(encoder_fw_output, encoder_bw_output), (encoder_fw_state, encoder_bw_state) = \
		tf.nn.bidirectional_dynamic_rnn(
		forward_cell, backward_cell, tokens_embedded, time_major=False, dtype=tf.float32)

		#Concatenate and halve sizes of both c and h
		encoder_state_h = tf.concat([encoder_fw_state.h,encoder_bw_state.h], axis = 1)
		encoder_state_h_half = tf.layers.dense(inputs=encoder_state_h, units=decsize, activation=tf.nn.tanh)
		encoder_state_c = tf.concat([encoder_fw_state.c,encoder_bw_state.c], axis = 1)
		encoder_state_c_half = tf.layers.dense(inputs=encoder_state_c, units=decsize, activation=tf.nn.tanh)
		encoder_state = tf.nn.rnn_cell.LSTMStateTuple(encoder_state_c_half, encoder_state_h_half)

		#Concatenate outputs
		encoder_output = tf.concat([encoder_fw_output,encoder_bw_output], axis = 2)
		

#########################################################################




####################### DECODER #########################################
with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):
	#Projection after decoder
	projection_layer = layers_core.Dense(target_vocab_size, use_bias=False)
	
	#Output Embeddings
	output_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, outembsize], -1.0, 1.0), dtype=tf.float32)
	summaries_dec_embedded = tf.nn.embedding_lookup(output_embeddings, summaries_dec)
	
	# Build RNN cell
	decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(decsize, state_is_tuple=True, name="d")
	# Helper
	helper = tf.contrib.seq2seq.TrainingHelper(summaries_dec_embedded, summary_lengths, time_major=False)
	#helper = tf.contrib.seq2seq.TrainingHelper(summaries_dec, tf.fill([None], max_target_length), time_major=False)
	
	if(attention == True):
	####################### ATTENTION #########################################
		if(bidirectional == False):
			attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(encsize, encoder_output)
			decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=encsize)
			
		else:
			attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(
		2*encsize, encoder_output)
			decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=2*encsize)
			
		# Decoder
		decoder_initial_state_ = decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size)	
		decoder_initial_state = decoder_initial_state_.clone(cell_state=encoder_state)
	#########################################################################
	else:
		decoder_initial_state = encoder_state



	decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, decoder_initial_state, output_layer=projection_layer)
	# Dynamic decoding
	outputs, _, _= tf.contrib.seq2seq.dynamic_decode(decoder)
	logits = outputs.rnn_output
	#print("logits", logits.shape)
#########################################################################


#Softmax
softmax = tf.nn.softmax(logits)


#Trim targets to maximum length of batch size
#Maximum of second dimension of summary_lengths is max_target_length_in_batch
max_target_length_in_batch = tf.reduce_max(summary_lengths)
#Slice second dimension to max_target_length_in_batch
summaries_loss_trimmed = tf.slice(summaries_loss, [0, 0], [num_instances, max_target_length_in_batch])
target_weights_trimmed = tf.slice(target_weights, [0, 0], [num_instances, max_target_length_in_batch])
#Loss
crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=summaries_loss_trimmed,
														 logits=softmax)
loss_calc = (tf.reduce_sum(crossent*target_weights_trimmed)/tf.to_float(num_instances))


# Calculate and clip gradients
params = tf.trainable_variables()
gradients = tf.gradients(loss_calc, params)
clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)
# Optimization
optimizer = tf.train.AdamOptimizer(lr)
update_step = optimizer.apply_gradients(zip(clipped_gradients, params))


#Shuffler
batch_shuffle = tf.train.shuffle_batch([train_input_tokens_id, train_target_tokens_id, train_target_lengths,
	shifted_train_target_tokens_id, train_target_weights], 
	enqueue_many=True, batch_size=batch_size, capacity=batch_size+1, 
	min_after_dequeue=batch_size, allow_smaller_final_batch=True)


########################### INFERENCE  #########################################
with tf.variable_scope('inference', reuse=tf.AUTO_REUSE):
	# Build RNN cellf
	inference_cell = tf.nn.rnn_cell.BasicLSTMCell(decsize, state_is_tuple=True, name="i")
	maximum_iterations = tf.round(tf.reduce_max(max_target_length) * 2)
		
	if decode_method == "greedy":
		#GREEDY
		# Helper
		greedy_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(output_embeddings,
		tf.fill([num_instances], start_symbol_id), end_symbol_id)

		# Decoder
		greedy_decoder = tf.contrib.seq2seq.BasicDecoder(
		inference_cell, greedy_helper, decoder_initial_state,
		output_layer=projection_layer)
		# Dynamic decoding
		greedy_outputs, _, greedy_lengths  = tf.contrib.seq2seq.dynamic_decode(greedy_decoder, 
												maximum_iterations=maximum_iterations)
		inference_ids = greedy_outputs.sample_id
		inference_lengths = greedy_lengths
		greedy_logits = greedy_outputs.rnn_output

	else:
		#BEAM SEARCH
		# Replicate encoder infos beam_width times
		beam_decoder_initial_state = tf.contrib.seq2seq.tile_batch(
		decoder_initial_state, multiplier=beam_width)
		
		# Define a beam-search decoder
		beam_decoder = tf.contrib.seq2seq.BeamSearchDecoder(
		cell=inference_cell,
		embedding=output_embeddings,
		start_tokens= tf.fill([num_instances], start_symbol_id),
		end_token=end_symbol_id,
		initial_state=beam_decoder_initial_state,
		beam_width=beam_width,
		output_layer=projection_layer,
		length_penalty_weight=0.0)

		# Dynamic decoding
		beam_outputs, _ , beam_lengths = tf.contrib.seq2seq.dynamic_decode(beam_decoder, maximum_iterations=maximum_iterations)
		inference_ids = beam_outputs.predicted_ids
		inference_lengths = beam_lengths
###############################################################################

"""### **RUN TRAINING**"""

# #PARSE ARGUMENTS
# parser = argparse.ArgumentParser(description='RNN')
# parser.add_argument('--lr', action="store", dest = 'lr', type=float)
# parser.add_argument('--batch_size', action="store", dest="batch_size", type=int)
# parser.add_argument('--init', action="store", dest="init", type=int)
# parser.add_argument('--save_dir', action="store", dest="save_dir")
# parser.add_argument('--dropout_prob', action="store", dest="dropout_prob")
# parser.add_argument('--decode_method', action="store", dest="decode_method")
# parser.add_argument('--beam_width', action="store", dest="beam_width")
# args = parser.parse_args()
# print(args)
# if(args.lr):
# 	learning_rate = args.lr
# if(args.batch_size):
# 	if(not(mini_batch_size == 1 or mini_batch_size%5 == 0)):
# 		raise ValueError('Valid values for batch_size are 1 and multiples of 5 only')
# 	else:
# 		mini_batch_size = args.batch_size	
# if(args.init):
# 	init_method = args.init
# if(args.save_dir):
# 	save_dir = args.save_dir
# if(args.dropout_prob):
# 	dropout_prob = args.dropout_prob
# if(args.decode_method):
# 	decode_method = args.decode_method
# if(args.beam_width):
# 	beam_width = args.beam_width




################## RUN ######################################################
last_five_val_losses = np.zeros(5)
prev_loss = np.inf
loss_file = open("losses", 'w')
with tf.Session() as sess:
	
	sess.run(tf.global_variables_initializer())
	step = 0
	coord = tf.train.Coordinator()
	threads = tf.train.start_queue_runners(coord=coord)		

	for e in range(epochs):	
		total_loss_train = 0	
		print("\nEpoch " + str(e+1))

		for i in range(0, train_input_tokens_id.shape[0], batch_size):
			#print(step)
			step += 1
			train_input_tokens_id_batch, train_target_tokens_id_batch, train_target_lengths_batch,\
			shifted_train_target_tokens_id_batch, train_target_weights_batch = sess.run(batch_shuffle)			
			
			logitsout, loss_train, _ = sess.run([logits, loss_calc, update_step], feed_dict = 
												{tokens : train_input_tokens_id_batch, 
												summaries_dec : train_target_tokens_id_batch,
												summary_lengths : train_target_lengths_batch,
												summaries_loss : shifted_train_target_tokens_id_batch,
												target_weights : train_target_weights_batch,
												num_instances : batch_size, 
												lr : learning_rate})
			total_loss_train += loss_train*batch_size	
			print(loss_train)

		
		loss_val = sess.run(loss_calc, feed_dict = {tokens : valid_input_tokens_id, 
												summaries_dec : valid_target_tokens_id,
												summary_lengths : valid_target_lengths,
												summaries_loss : shifted_valid_target_tokens_id,
												num_instances : valid_input_tokens_one_hot.shape[0],
												target_weights : valid_target_weights})
		
		print("Training :  Loss= {:.6f}".format(total_loss_train))		
		print("Validation : Loss= {:.6f}".format(loss_val))
		loss_file.write(str(e+1)+" "+str(total_loss_train)+" "+str(loss_val)+"\n")
		


		if (anneal == True) or (early_stopping == True):
			save_path = saver.save(sess, save_dir+"/"+str(e)+".ckpt")
		if(anneal == True):
			if(loss_val > prev_loss):
				learning_rate = learning_rate/2.0
				saver.restore(sess, save_dir+"/"+str(max(0,e-1))+".ckpt")
			else:
				prev_loss = loss_val	
		if(early_stopping == True):
			np.roll(last_five_val_losses, -1)
			last_five_val_losses[-1] = loss_val
			if loss_val > last_five_val_losses[0]:
				saver.restore(sess, save_dir+"/"+str(max(0,e-5))+".ckpt")
				break

	coord.request_stop()
	coord.join(threads)
	loss_file.close()
	print("Training Done")

	
	"""### **RUN TEST**"""

	#Train output
	# train_inf_summaries, train_inf_lengths = sess.run([inference_ids, inference_lengths], feed_dict = {tokens : train_input_tokens_one_hot,
	# 											num_instances : train_input_tokens_one_hot.shape[0],
	# 											output_embedding_weights : trained_output_embeddings})
	# print(train_inf_summaries[0].shape, train_inf_lengths.shape))

	#Val output
	val_inf_summaries, val_inf_lengths = sess.run([inference_ids, inference_lengths], feed_dict = {tokens : valid_input_tokens_id,
												num_instances : valid_input_tokens_one_hot.shape[0]})
	print(val_inf_summaries.shape, val_inf_lengths.shape)

	#Test output
	test_inf_summaries, test_inf_lengths = sess.run([inference_ids, inference_lengths], feed_dict = {tokens : test_input_tokens_id,
												num_instances : test_input_tokens_one_hot.shape[0]})
	print(test_inf_summaries.shape, test_inf_lengths.shape)

	


	# with open(save_dir + "train_output", 'w') as train_output_file:
	# 	train_inf_summaries = train_inf_summaries[0]
	# 	if(decode_method == "greedy"):
	# 		for train_inf_summary in train_inf_summaries:
	# 			for word_id in train_inf_summary:
	# 			train_output_file.write(target_id_to_word.get(word_id, "unk") + " ")
	# 		train_output_file.write("\n")
	# 	else:
	# 		for train_inf_summary in train_inf_summaries:
	# 			for word_id in train_inf_summary:
	# 			train_output_file.write(target_id_to_word.get(word_id[0], "unk") + " ")
	# 		train_output_file.write("\n")
	# os.system("python bleuscript/pycocoevalcap/eval.py WeatherGov/train/summaries.txt " + save_dir + "train_output")

	d=  {}
	li_summ = []
	with open(save_dir + "val_output", 'w') as val_output_file:
		# val_inf_summaries = val_inf_summaries[0]
		if(decode_method == "greedy"):
			# for val_inf_summary in val_inf_summaries:
			# 	for word_id in val_inf_summary:
			# 		val_output_file.write(target_id_to_word.get(word_id, "unk") + " ")
			# 	val_output_file.write("\n")
			for i in len(val_inf_summaries):
				summ = ''
				d['image_id'] = i
				for word_id in val_inf_summaries[i]:
					summ =summ + target_id_to_word.get(word_id, "unk") + " "
				summ = summ + '\n'	
				d['caption'] = summ
				li_summ.append(d)
		else:
			# for val_inf_summary in val_inf_summaries:
			# 	for word_id in val_inf_summary:
			# 		val_output_file.write(target_id_to_word.get(word_id[0], "unk") + " ")
			# 	val_output_file.write("\n")
			for i in len(val_inf_summaries):
				summ = ''
				d['image_id'] = i
				for word_id in val_inf_summaries[i]:
					summ =summ + target_id_to_word.get(word_id[0], "unk") + " "
				summ = summ + '\n'	
				d['caption'] = summ
				li_summ.append(d)
		json.dump(li_summ,test_output_file,indent=4)
	#os.system("python bleuscript/pycocoevalcap/eval.py WeatherGov/val/summaries.txt " + save_dir + "val_output")
	li_summ = []
	with open(save_dir + "test_output", 'w') as test_output_file:
		# test_inf_summaries = test_inf_summaries[0]
		if(decode_method == "greedy"):
			# for test_inf_summary in test_inf_summaries:
			# 	for word_id in test_inf_summary:
			# 		test_output_file.write(target_id_to_word.get(word_id, "unk") + " ")
			# 	test_output_file.write("\n")
			for i in len(test_inf_summaries):
				summ = ''
				d['image_id'] = i
				for word_id in test_inf_summaries[i]:
					summ =summ + target_id_to_word.get(word_id, "unk") + " "
				summ = summ + '\n'	
				d['caption'] = summ
				li_summ.append(d)
		else:
			# for test_inf_summary in test_inf_summaries:
			# 	for word_id in test_inf_summary:
			# 		test_output_file.write(target_id_to_word.get(word_id[0], "unk") + " ")
			# 	test_output_file.write("\n")
			for i in len(test_inf_summaries):
				summ = ''
				d['image_id'] = i
				for word_id in test_inf_summaries[i]:
					summ =summ + target_id_to_word.get(word_id[0], "unk") + " "
				summ = summ + '\n'	
				d['caption'] = summ
				li_summ.append(d)
		json.dump(li_summ,test_output_file,indent=4)	